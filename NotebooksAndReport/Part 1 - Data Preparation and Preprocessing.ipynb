{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3803ICT Big Data Analysis Assignment\n",
    "\n",
    "Joshua Russell | s5057545 | joshua.russell2@griffithuni.edu.au \n",
    "\n",
    "Joshua Mitchell | s5055278 | joshua.mitchell4@griffithuni.edu.au\n",
    "\n",
    "\n",
    "## Part 1 - Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Data preparation and preprocessing involves three main processes; data storage, data normailzation and data cleaning. We will therefore introduce and discuss these concepts here.\n",
    "\n",
    "#### Data Storage\n",
    "Data storage in concerned with the way in which data is stored. Within this investigation, the Python library Pandas will be the primary method used for storing, processing and manipulating data. \n",
    "\n",
    "#### Data Normailzation\n",
    "Within the context of this course, data normailzation encapsulates data scaling. Consequently, scaling will be discussed as a data normailzation technique. However, there is two important distingushing factors that should be outlined. Data scaling is the process by which data is *scaled* into a specific range, such as between 0 and 1. The meaning behind this transformation is to value numerical features with the same importance when using models that implement distance or error functions. To provide more intuition behind this matter, we will discuss an example. \n",
    "\n",
    "Say we were analysing the salaries of American university professors with those of Korean professors. For simplicity, we will state that the exchange between USD and Korean won is 1 to 1000. If we were to use an analysis method such as KNN with euclidean distance as the distance function, 1 USD would be weighted the same as 1 Korean won. Our distance function would consequently be misrepresenting the relationship between the two currencies, giving a *bias* to one of the variables. We therefore scale our data such that variables can be properly compared with one another.\n",
    "\n",
    "Data scaling involves the transformation of the scale of variables into a specific range. The process on data normalization is concerned with a much broader view, looking at how samples are distributed among the entire dataset. Many statistical analysis techniques assume that the data is normally distributed. Datasets that contain skewed or other non-normal distributions of numerical data will need to be transformed into normal distributions such that these techniques can provide meaningful insights about the data. This process of transforming data into a distribution that can be regarded as being normal is data normalization. \n",
    "\n",
    "#### Data Cleaning\n",
    "The process of data cleaning involves finding data with incorrect formatting, data with missing values, outliers, data that is erroneous, inconsistent, irrelevant and malicious, and either correcting or removing those samples. The types of data mentioned here are collectively known as \"dirty\" data. We will briefly describe the types of dirty data below.\n",
    "\n",
    "- **Formatting**: data containing the same information with inconsistent formatting\n",
    "- **Missing data**: data samples with missing variable information\n",
    "- **Erroneous data**: data that is erroneous with respect to other samples or in regard to the context of the information\n",
    "- **Irrelevant data**: data that does not affect the results of statistical or analitical methods\n",
    "- **Inconsistent data**: data that can be represented in two or more ways\n",
    "- **Malicious data**: data designed to cause damage or an undesired effect\n",
    "- **Outliers**: data that is deviated from or inconsistent with other data in the dataset, also referred to as noise or anomolies\n",
    "\n",
    "These three concepts will be elaborated on and used in the following investigation while preparing and preprocessing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Increase the width of the notebook for displaying DataFrames\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Known dependencies:\n",
    "\n",
    "`pandas-0.23.4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the job market dataset into a pandas DataFrame\n",
    "df = pd.read_csv(\"data.csv\", dtype={\"Id\": np.str, \"Location\": np.str, \"Area\": np.str, \n",
    "                                    \"Classification\": np.str, \"SubClassification\": np.str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Describing the Dataset\n",
    "\n",
    "SEEK is a company that facilitates a platform for jobseekers to find jobs, and for employers to find employees. More specifically, they host one of Australia's largest employment marketplaces online under the url `www.seek.com.au`.\n",
    "\n",
    "The provided dataset, which we will be using within this investigation, is of the SEEK job marketplace. Where each row is a job offering, identified by an Id, with information about the details, specifications and requirements of the job. Besides the job Id and Salary information, all data within the dataset is textual. Salary information is split into two columns, *LowestSalary* and *HighestSalary*, which are the only informative numerical columns in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>Area</th>\n",
       "      <th>Classification</th>\n",
       "      <th>SubClassification</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LowestSalary</th>\n",
       "      <th>HighestSalary</th>\n",
       "      <th>JobType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37404348</td>\n",
       "      <td>Casual Stock Replenisher</td>\n",
       "      <td>Aldi Stores</td>\n",
       "      <td>2018-10-07T00:00:00.000Z</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>North West &amp; Hills District</td>\n",
       "      <td>Retail &amp; Consumer Products</td>\n",
       "      <td>Retail Assistants</td>\n",
       "      <td>Our Casual Stock Replenishers pride themselves...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37404337</td>\n",
       "      <td>Casual Stock Replenisher</td>\n",
       "      <td>Aldi Stores</td>\n",
       "      <td>2018-10-07T00:00:00.000Z</td>\n",
       "      <td>Richmond &amp; Hawkesbury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Retail &amp; Consumer Products</td>\n",
       "      <td>Retail Assistants</td>\n",
       "      <td>Our Casual Stock Replenishers pride themselves...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37404356</td>\n",
       "      <td>RETAIL SALES SUPERSTARS and STYLISTS Wanted - ...</td>\n",
       "      <td>LB Creative Pty Ltd</td>\n",
       "      <td>2018-10-07T00:00:00.000Z</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>CBD &amp; Inner Suburbs</td>\n",
       "      <td>Retail &amp; Consumer Products</td>\n",
       "      <td>Retail Assistants</td>\n",
       "      <td>BRAND NEW FLAGSHIP STORE OPENING - SUNSHINE PLAZA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37404330</td>\n",
       "      <td>Team member - Belrose</td>\n",
       "      <td>Anaconda Group Pty Ltd</td>\n",
       "      <td>2018-10-07T00:00:00.000Z</td>\n",
       "      <td>Gosford &amp; Central Coast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Retail &amp; Consumer Products</td>\n",
       "      <td>Retail Assistants</td>\n",
       "      <td>Bring it on - do you love the great outdoors a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37404308</td>\n",
       "      <td>Business Banking Contact Centre Specialist, Ni...</td>\n",
       "      <td>Commonwealth Bank - Business &amp; Private Banking</td>\n",
       "      <td>2018-10-07T00:00:00.000Z</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Ryde &amp; Macquarie Park</td>\n",
       "      <td>Call Centre &amp; Customer Service</td>\n",
       "      <td>Sales - Inbound</td>\n",
       "      <td>We are seeking highly articulate, enthusiastic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  37404348                           Casual Stock Replenisher   \n",
       "1  37404337                           Casual Stock Replenisher   \n",
       "2  37404356  RETAIL SALES SUPERSTARS and STYLISTS Wanted - ...   \n",
       "3  37404330                              Team member - Belrose   \n",
       "4  37404308  Business Banking Contact Centre Specialist, Ni...   \n",
       "\n",
       "                                          Company                      Date  \\\n",
       "0                                     Aldi Stores  2018-10-07T00:00:00.000Z   \n",
       "1                                     Aldi Stores  2018-10-07T00:00:00.000Z   \n",
       "2                             LB Creative Pty Ltd  2018-10-07T00:00:00.000Z   \n",
       "3                          Anaconda Group Pty Ltd  2018-10-07T00:00:00.000Z   \n",
       "4  Commonwealth Bank - Business & Private Banking  2018-10-07T00:00:00.000Z   \n",
       "\n",
       "                  Location                         Area  \\\n",
       "0                   Sydney  North West & Hills District   \n",
       "1    Richmond & Hawkesbury                          NaN   \n",
       "2                 Brisbane          CBD & Inner Suburbs   \n",
       "3  Gosford & Central Coast                          NaN   \n",
       "4                   Sydney        Ryde & Macquarie Park   \n",
       "\n",
       "                   Classification  SubClassification  \\\n",
       "0      Retail & Consumer Products  Retail Assistants   \n",
       "1      Retail & Consumer Products  Retail Assistants   \n",
       "2      Retail & Consumer Products  Retail Assistants   \n",
       "3      Retail & Consumer Products  Retail Assistants   \n",
       "4  Call Centre & Customer Service    Sales - Inbound   \n",
       "\n",
       "                                         Requirement FullDescription  \\\n",
       "0  Our Casual Stock Replenishers pride themselves...             NaN   \n",
       "1  Our Casual Stock Replenishers pride themselves...             NaN   \n",
       "2  BRAND NEW FLAGSHIP STORE OPENING - SUNSHINE PLAZA             NaN   \n",
       "3  Bring it on - do you love the great outdoors a...             NaN   \n",
       "4  We are seeking highly articulate, enthusiastic...             NaN   \n",
       "\n",
       "   LowestSalary  HighestSalary JobType  \n",
       "0             0             30     NaN  \n",
       "1             0             30     NaN  \n",
       "2             0             30     NaN  \n",
       "3             0             30     NaN  \n",
       "4             0             30     NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the categories/domains of the dataset?\n",
    "\n",
    "This question is answered under the assumption that the categories/domains/variations of a dataset refer to the broad field in which the data is from. For example, the domain of a dataset of hospital patients would be healthcare, and the domain of a dataset containing information about stars and galaxies would be astronomy. \n",
    "\n",
    "Under our assumption, the domain of the SEEK dataset is employment marketplace. Where job offers are posted by companies and organisations for jobseekers to find.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the dataset size of each variation?\n",
    "\n",
    "Based on the assumption stated above, our dataset consists of only one variation (namely the employment marketplace). Therefore, the dataset size of the employment marketplace variation can be found via the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of SEEK dataset (employment marketplace variation): 318477 job listings\n"
     ]
    }
   ],
   "source": [
    "# Print the dataset size of each variation\n",
    "print(\"Size of SEEK dataset (employment marketplace variation): {} job listings\".format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the dataset structure/format? \n",
    "\n",
    "The dataset structure is that of a job description containing all of its listing information. More specifically, each row in the dataset is a job listing described by the following attributes: job title, company name, date, location, area within the location, general classification of the job and its specific sub-classification, requirements, full description, and the lowest and highest salary for the respective sub-classification.\n",
    "\n",
    "The job title, location, area, classification and sub-classification are short and consistent strings that are part of a relatively small set of values. These values are likely taken from the SEEK websites selection of values. The company name and date are short strings as well.\n",
    "\n",
    "Requirements and the full description of the job listing are longer strings, where the full description contains the raw html of the job description. They contain whitespace characters, unicode emojis and bulletpoint values. They are mostly inconsistent and are mostly all unique for each row.\n",
    "\n",
    "The lowest salary and highest salary columns consist of 11 different integer values for each salary increment. The job type column contains 4 string values categorising each job listing as either part time, full time, contract/temp, or casual/vacation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column               Datatype\n",
      "------               --------\n",
      "Id                   object\n",
      "Title                object\n",
      "Company              object\n",
      "Date                 object\n",
      "Location             object\n",
      "Area                 object\n",
      "Classification       object\n",
      "SubClassification    object\n",
      "Requirement          object\n",
      "FullDescription      object\n",
      "LowestSalary         int64\n",
      "HighestSalary        int64\n",
      "JobType              object\n"
     ]
    }
   ],
   "source": [
    "# Columns of the raw dataset\n",
    "print(\"{:<20} {}\".format(\"Column\", \"Datatype\"))\n",
    "print(\"{:<20} {}\".format(\"------\", \"--------\"))\n",
    "for col in np.c_[list(df.columns), list(df.dtypes)]:\n",
    "    print(\"{:<20} {}\".format(col[0], col[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which parts of the dataset will be used? \n",
    "\n",
    "We hypothesise that we will be using all of the columns within the dataset for analysis and visulisation except for the id column which is irrelevant for our investigation purposes, as it does not offer anything significant in regard to job listings. However, since it uniquely identifies rows it will be kept for when operating with PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions in regard to describing the dataset\n",
    "\n",
    "The dataset to be used for this investigation is from the company SEEK's website. The website provides a platform for jobseeksers to find employers and employers to find their employees. Consequently, the website consists of a collection of job advestisement listings containing information about particular jobs. A crawler was used to collect this information and form the dataset to be used. \n",
    "\n",
    "In more detail, the dataset contains 318,477 job listings, each of which is described by the following attributes: id, title, company, date, location, area, classification, sub-classification, requirement, full description, lowest salary, highest salary, and job type. Due to the fact that this information is still raw within the dataset, since it assumedly has not been processed after being collected by the crawler, the data is dirty. Accordingly, the following section will describe the steps involved in preprocessing the data and preparing it for use in the analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Steps used for Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset with Pandas\n",
    "\n",
    "We use the Pandas function `pandas.read_csv()` to load the data into the notebook. The raw dataset contained inconsistent representations of the columns \"Id\", \"Location\", \"Area\", \"Classification\" and \"SubClassification\". In the case of \"Id\", the regular Id values were 8 numbers long and therefore interpreted as integers. However, there we 43462 job advertisement samples with Id values followed by a string of characters. For example:\n",
    "\n",
    "```\n",
    "       Valid ID: 37915260\n",
    "Inconsistent ID: 37915260&searchrequesttoken=e859cc74-e22f-498d-ac7c-77a7e1b45676   \n",
    "```\n",
    "\n",
    "Consequently, Pandas was not able to read the the \"Id\" column values as integers and therefore stored them as objects (str). \n",
    "\n",
    "We outline the specific function and parameters used to read the dataset below:\n",
    "```\n",
    "df = pd.read_csv(\"data.csv\", dtype={\"Id\": np.str, \"Location\": np.str, \"Area\": np.str, \"Classification\": np.str, \"SubClassification\": np.str})\n",
    "```\n",
    "\n",
    "For the other columns that raised the DtypeWarning it is assumed that their rows contained nan values of type float. As we found such occurrences while preprocessing the string data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalisation\n",
    "\n",
    "As mentioned within *Introduction*, data normalization in the context of this course entails both normalizing the data distribution and scaling the data into a specific range. These normalization techniques are for numerical data. We therefore consider the numerical columns of the dataset, *Lowest Salary* and *Highest Salary*, and investigate the importance of normailzing these distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LowestSalary range:  0 - 200\n",
      "HighestSalary range: 30 - 999\n"
     ]
    }
   ],
   "source": [
    "# Extract series from the DataFrame\n",
    "lowest_salary_series = np.array(df[\"LowestSalary\"].values)\n",
    "highest_salary_series = np.array(df[\"HighestSalary\"].values)\n",
    "\n",
    "# Find min and max values of the two series\n",
    "lowest_salary_min = min(lowest_salary_series)\n",
    "lowest_salary_max = max(lowest_salary_series)\n",
    "highest_salary_min = min(highest_salary_series)\n",
    "highest_salary_max = max(highest_salary_series)\n",
    "\n",
    "print(\" LowestSalary range:  {} - {}\".format(lowest_salary_min, lowest_salary_max))\n",
    "print(\"HighestSalary range: {} - {}\".format(highest_salary_min, highest_salary_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LowestSalaryScaled range: 0.00 - 0.20\n",
      "HighestSalaryScaled range: 0.03 - 1.00\n"
     ]
    }
   ],
   "source": [
    "# Scale Lowest Salary values into the range of 0.0 - 1.0\n",
    "lowest_salary_scaled_series = np.zeros(lowest_salary_series.shape)\n",
    "for i, val in enumerate(lowest_salary_series):\n",
    "    lowest_salary_scaled_series[i] = (val - lowest_salary_min) / (highest_salary_max - lowest_salary_min)\n",
    "    \n",
    "# Scale Highest Salary values into the range of 0.0 - 1.0\n",
    "highest_salary_scaled_series = np.zeros(highest_salary_series.shape)\n",
    "for i, val in enumerate(highest_salary_series):\n",
    "    highest_salary_scaled_series[i] = (val - lowest_salary_min) / (highest_salary_max - lowest_salary_min)\n",
    "\n",
    "# Add the two scaled series to the DataFrame\n",
    "df = df.assign(LowestSalaryScaled=lowest_salary_scaled_series)\n",
    "df = df.assign(HighestSalaryScaled=highest_salary_scaled_series)\n",
    "\n",
    "# Extract series from the DataFrame\n",
    "lowest_salary_scaled_series = np.array(df[\"LowestSalaryScaled\"].values)\n",
    "highest_salary_scaled_series = np.array(df[\"HighestSalaryScaled\"].values)\n",
    "\n",
    "# Find min and max values of the two series\n",
    "lowest_salary_scaled_min = min(lowest_salary_scaled_series)\n",
    "lowest_salary_scaled_max = max(lowest_salary_scaled_series)\n",
    "highest_salary_scaled_min = min(highest_salary_scaled_series)\n",
    "highest_salary_scaled_max = max(highest_salary_scaled_series)\n",
    "\n",
    "print(\" LowestSalaryScaled range: {:.2f} - {:.2f}\".format(lowest_salary_scaled_min, lowest_salary_scaled_max))\n",
    "print(\"HighestSalaryScaled range: {:.2f} - {:.2f}\".format(highest_salary_scaled_min, highest_salary_scaled_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the investigation above, we found that the ranges of numerical data for the two columns in consideration were 0-200 and 30-999 for *Lowest Salary* and *Highest Salary*, respectively. It is assumed that the magnitude of this scale is slightly too large for distance fuctions in statistical analysis techniques. However, transforming the numerical scale of salary will remove the interpretable nature of the data which will be needed for data visualization. Consequenty, we decided to add two new columns to the dataset for the scaled lowest salary values (*LowestSalaryScaled*) and the scaled highest salary values (*HighestSalaryScaled*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "Within the *Introduction*, we outlined that data cleaning entails correcting and/or removing dirty data from the dataset. In the data cleaning analysis that follows, we clean the Id values, format the date samples, set the data type of columns in the DataFrame, and clean all textual data. Further reasoning is provided within each section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Id values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned within *Loading the dataset with Pandas*, the raw dataset values of the \"Id\" column had inconsistencies in their representation. Most Id values were 8 number long integers, however there were other job advertisements with valid Id values followed by a string of characters. \n",
    "\n",
    "We identified that all of the strings following the Id values started within an ampersand ('&'). This allowed us to select the string of characters after the valid Id number using regular expressions, and remove them with the Pandas fuction `pandas.Series.replace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Id\"] = df[\"Id\"].replace(to_replace=r'&.*', value='', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Formatting Date values\n",
    "\n",
    "The raw dataset values of the \"Date\" column were represented in a format that contained both date and time information of when the advertisements were uploaded to SEEK. The website does not display specific time information, only showing the date. As a result, all time values were `00:00:00.000`. This information was consequently thought to be redundant. \n",
    "\n",
    "We identified that the Date values were all formatted in the following way:\n",
    "\n",
    "`2018-10-07T00:00:00.000Z`\n",
    "\n",
    "Where the date and time values were separated by the character 'T'. To remove the time value, we used regular expressions to select the 'T' as well as all subsequent characters within the string, and then replaced them with an empty string using the Pandas function `pandas.Series.replace`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = df[\"Date\"].replace(to_replace=r'T.*', value='', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the data types of columns\n",
    "\n",
    "We changed the data type of the column \"Date\" to datetime64[ns]. This was required due to formatting and consistency errors in the raw dataset. Furthermore, the data type conversion was made to provide a consistent representation of the data and allow for the use of functions and methods that process these data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                      object\n",
       "Title                   object\n",
       "Company                 object\n",
       "Date                    object\n",
       "Location                object\n",
       "Area                    object\n",
       "Classification          object\n",
       "SubClassification       object\n",
       "Requirement             object\n",
       "FullDescription         object\n",
       "LowestSalary             int64\n",
       "HighestSalary            int64\n",
       "JobType                 object\n",
       "LowestSalaryScaled     float64\n",
       "HighestSalaryScaled    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data types of raw dataset\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set column \"Id\" to type int64\n",
    "df[\"Id\"] = pd.to_numeric(df[\"Id\"])\n",
    "\n",
    "# Set column \"Date\" to type datetime64[ns]\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                              int64\n",
       "Title                          object\n",
       "Company                        object\n",
       "Date                   datetime64[ns]\n",
       "Location                       object\n",
       "Area                           object\n",
       "Classification                 object\n",
       "SubClassification              object\n",
       "Requirement                    object\n",
       "FullDescription                object\n",
       "LowestSalary                    int64\n",
       "HighestSalary                   int64\n",
       "JobType                        object\n",
       "LowestSalaryScaled            float64\n",
       "HighestSalaryScaled           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data types after data cleaning and conversions\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning textual data\n",
    "\n",
    "Raw textual data contains whitespace characters, unicode for special characters and emojis, and naturally, punctuation. Punctuation is useful for understanding a language, when the context and structure behind the words needs to be extracted. Within this investigation, we will not be investigating the meaning of the sentences as a whole, rather the meaning behind specific words. Consequently, punctuation, whitespace characters and special characters will not provide anything towards our analysis. As a result, we decided to remove these characters from our dataset to increase the efficiency of processing textual data during our anaylsis. In addition to the removal of the aforementioned characters, we replaced NaN values with empty strings and set all characters to lower case for the same reason as to increase our processing efficiency. The only exception to this was that the \"FullDescription\" column was left as it is, without any cleaning besides removing NaN values. This decision was made to preserve the HTML syntax found within the text of this attribute. The HTML syntax was later used in the analysis for parsing and was consequently left as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column               null count\n",
      "Id                   0\n",
      "Title                0\n",
      "Company              12004\n",
      "Date                 0\n",
      "Location             121248\n",
      "Area                 195819\n",
      "Classification       121248\n",
      "SubClassification    121248\n",
      "Requirement          7\n",
      "FullDescription      16175\n",
      "LowestSalary         0\n",
      "HighestSalary        0\n",
      "JobType              16098\n",
      "LowestSalaryScaled   0\n",
      "HighestSalaryScaled  0\n"
     ]
    }
   ],
   "source": [
    "# Print the number of null counts per dataset column\n",
    "print(\"{:<20} {}\".format(\"Column\", \"null count\"))\n",
    "for col in df.columns:\n",
    "    print(\"{:<20} {}\".format(col, df[col].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column in the DataFrame\n",
    "for col in df.columns:\n",
    "    \n",
    "    # For each column of data type string \n",
    "    # Note: the \"FullDescription column is left unchanged so that the HTML can be parsed in the analysis section\n",
    "    if df[col].dtype == \"object\" and col != \"FullDescription\":\n",
    "        \n",
    "        # Clean whitespace characters\n",
    "        df[col] = df[col].replace(to_replace=['\\n', '\\t', '\\r'], value='')\n",
    "        \n",
    "        # Clean HTML unicode\n",
    "        df[col] = df[col].replace(to_replace=[r'&#\\d\\d\\d\\d'], value='', regex=True)\n",
    "        \n",
    "        # Clean punctuation, replace NaN values with an empty string, and set all characters to lower case\n",
    "        for r, row in enumerate(df[col].values):\n",
    "                \n",
    "            row = str(row)\n",
    "            row = row.lower()\n",
    "            \n",
    "            if row == \"nan\" or pd.isnull(row):\n",
    "                row = \"\"\n",
    "                \n",
    "            row = row.translate(str.maketrans('', '', string.punctuation))\n",
    "            \n",
    "            # Set the whitespace between words to be a single space\n",
    "            tokens = row.split()\n",
    "            row = \" \".join(tokens)\n",
    "            \n",
    "            df.at[r, col] = row\n",
    "            \n",
    "# Replace NaN values with an empty string for the FullDescription column\n",
    "for r, row in enumerate(df[\"FullDescription\"].values):\n",
    "    if row == \"nan\" or pd.isnull(row):\n",
    "        row = \"\"\n",
    "        df.at[r, \"FullDescription\"] = row\n",
    "\n",
    "# Replace emojis with empty strings in the titles\n",
    "emoji_regex_string = r'(ðŸ’|ðŸ“£|ðŸ’¥|ðŸ‘—|ðŸ‘•|ðŸ›|ðŸ‘‰|ðŸ‘Ÿ|ðŸ†|ðŸ‘‘|ðŸŽ„|ðŸŽ|ðŸŽ‰|ðŸŽ—|ðŸŒŽ|âž•|âœ¨|â­ï¸|âš¡ï¸|â˜…|ðŸ–|â—ï¸)'\n",
    "df[\"Title\"] = df[\"Title\"].replace(to_replace=[emoji_regex_string], value=\"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column               null count\n",
      "Id                   0\n",
      "Title                0\n",
      "Company              0\n",
      "Date                 0\n",
      "Location             0\n",
      "Area                 0\n",
      "Classification       0\n",
      "SubClassification    0\n",
      "Requirement          0\n",
      "FullDescription      0\n",
      "LowestSalary         0\n",
      "HighestSalary        0\n",
      "JobType              0\n",
      "LowestSalaryScaled   0\n",
      "HighestSalaryScaled  0\n"
     ]
    }
   ],
   "source": [
    "# Print the number of null counts per dataset column to check they have been cleaned\n",
    "print(\"{:<20} {}\".format(\"Column\", \"null count\"))\n",
    "for col in df.columns:\n",
    "    print(\"{:<20} {}\".format(col, df[col].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>Area</th>\n",
       "      <th>Classification</th>\n",
       "      <th>SubClassification</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LowestSalary</th>\n",
       "      <th>HighestSalary</th>\n",
       "      <th>JobType</th>\n",
       "      <th>LowestSalaryScaled</th>\n",
       "      <th>HighestSalaryScaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>318472</th>\n",
       "      <td>38564891</td>\n",
       "      <td>program manager trade reporting</td>\n",
       "      <td>talenza</td>\n",
       "      <td>2019-03-13</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>our client is a large financial services firm ...</td>\n",
       "      <td></td>\n",
       "      <td>200</td>\n",
       "      <td>999</td>\n",
       "      <td></td>\n",
       "      <td>0.2002</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318473</th>\n",
       "      <td>38564940</td>\n",
       "      <td>web content writer june 2019 contract</td>\n",
       "      <td>talent â€“ winner â€˜seek large recruitment agency...</td>\n",
       "      <td>2019-03-13</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>you will be able to write clearly and concisel...</td>\n",
       "      <td></td>\n",
       "      <td>200</td>\n",
       "      <td>999</td>\n",
       "      <td></td>\n",
       "      <td>0.2002</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318474</th>\n",
       "      <td>38552964</td>\n",
       "      <td>brand director global premium brand</td>\n",
       "      <td>retail career consulting pty ltd</td>\n",
       "      <td>2019-03-12</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>newly created brand director aus must be comme...</td>\n",
       "      <td></td>\n",
       "      <td>200</td>\n",
       "      <td>999</td>\n",
       "      <td></td>\n",
       "      <td>0.2002</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318475</th>\n",
       "      <td>38534438</td>\n",
       "      <td>head of financial planning reporting strategy</td>\n",
       "      <td>moir group</td>\n",
       "      <td>2019-03-08</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>highquality manufacturing organisation excitin...</td>\n",
       "      <td></td>\n",
       "      <td>200</td>\n",
       "      <td>999</td>\n",
       "      <td></td>\n",
       "      <td>0.2002</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318476</th>\n",
       "      <td>38561751</td>\n",
       "      <td>head of operations eoi</td>\n",
       "      <td>austcorp executive</td>\n",
       "      <td>2019-03-13</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>expressions of interest for a leadership role</td>\n",
       "      <td></td>\n",
       "      <td>200</td>\n",
       "      <td>999</td>\n",
       "      <td></td>\n",
       "      <td>0.2002</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                                          Title  \\\n",
       "318472  38564891                program manager trade reporting   \n",
       "318473  38564940          web content writer june 2019 contract   \n",
       "318474  38552964            brand director global premium brand   \n",
       "318475  38534438  head of financial planning reporting strategy   \n",
       "318476  38561751                         head of operations eoi   \n",
       "\n",
       "                                                  Company       Date Location  \\\n",
       "318472                                            talenza 2019-03-13            \n",
       "318473  talent â€“ winner â€˜seek large recruitment agency... 2019-03-13            \n",
       "318474                   retail career consulting pty ltd 2019-03-12            \n",
       "318475                                         moir group 2019-03-08            \n",
       "318476                                 austcorp executive 2019-03-13            \n",
       "\n",
       "       Area Classification SubClassification  \\\n",
       "318472                                         \n",
       "318473                                         \n",
       "318474                                         \n",
       "318475                                         \n",
       "318476                                         \n",
       "\n",
       "                                              Requirement FullDescription  \\\n",
       "318472  our client is a large financial services firm ...                   \n",
       "318473  you will be able to write clearly and concisel...                   \n",
       "318474  newly created brand director aus must be comme...                   \n",
       "318475  highquality manufacturing organisation excitin...                   \n",
       "318476      expressions of interest for a leadership role                   \n",
       "\n",
       "        LowestSalary  HighestSalary JobType  LowestSalaryScaled  \\\n",
       "318472           200            999                      0.2002   \n",
       "318473           200            999                      0.2002   \n",
       "318474           200            999                      0.2002   \n",
       "318475           200            999                      0.2002   \n",
       "318476           200            999                      0.2002   \n",
       "\n",
       "        HighestSalaryScaled  \n",
       "318472                  1.0  \n",
       "318473                  1.0  \n",
       "318474                  1.0  \n",
       "318475                  1.0  \n",
       "318476                  1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and preprocessed dataset\n",
    "df.to_csv(\"preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions in regard to data preparation and preprocessing\n",
    "\n",
    "Within this section we applied data preparation and preprocessing techniques to the SEEK dataset in order to prepare the data for analysis and interpretation. The raw comma-separated values (CSV) file used for storing the dataset contained representation inconsistencies, where numerical data contained charater data, and character data assumedly contained numerical NaN values. Loading this dirty data therefore required telling pandas to load all the attributes as strings. These inconsistent representations were then cleaned in the subsequent preprocesssing steps. \n",
    "\n",
    "The lowest salary and highest salary attributes were the only attributes that contained meaningful numerical data in the dataset. We therefore applied data normalisation techniques to these attributes by scaling the existing range of 0 to 999 into the range of 0.00 to 1.00 such that statistical analysis techniques could be accurately used on the salary ranges.\n",
    "\n",
    "To correct and/or remove dirty data from the dataset, and to best prepare the data for data analysis and interpretation, data cleaning techniques were used. Firstly, redundant date representations were found in the dataset and cleaned through the use of regular expressions. Secondly, the id column was cleaned and converted into type int64, and the data type of the date column was converted from object (str) into datetime64[ns] such that functions and methods requiring this data type format could be used within the analysis. Lastly, all of the textual data within the dataset was cleaned and prepared for analysis by cleaning whitespace, punctuation and unicode characters, by replacing NaN values with empty strings, and by setting all characters to lower case. Additionally, specific emojies were replaced with empty strings in the text of the title attribute. Throughout this textual data cleaning process, the non-NaN text within the full description attribute was left unchanged. This decision was made to preserve the raw HTML contents of the text, such that it could be later utilised in the analysis for parsing specific information.\n",
    "\n",
    "These preparation and preprocessing techniques adequately prepared the data within the dataset for analysis and interpretation. Thus, the preprocessed dataset was saved to file so that it could be used within the analysis notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Hypotheses regarding the Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly consider an analysis on the location attribute of the dataset. When investigating the job market share across the cities of Australia, it is hypothesised that the majority of the market share will be in the major cities of Australia, with more rural and suburban areas having significantly less jobs advertised on the employment marketplace. Furthermore, we hypothesise that cities will have higher average salaries than other rural and suburban areas in Australia, since cities have competitive job markets that require employers to increase salaries in order to secure employees over competitors.\n",
    "\n",
    "In relation to the trends of the job market, we expect that the Information Communication Technology (ICT) sector has the largest market share among all sectors within the database. Moreover, we hypothesise that the average salary of the ICT sector has increased over the duration of the dataset, which is from October 2018 to March 2019, due to recent technological advancements and in turn investments in the ICT sector by companies and governmental organisations. When investigating the trends of posting job advertisements, we expect that the majority of jobs will be posted throughout the week, as employees generally are not working on weekends and therefore would not be posting job listings for their company.\n",
    "\n",
    "Regarding the salaries of job advertisements within sub-sectors, we hypothesise that jobs within sub-sectors requiring a higher level of expertise (such as Criminal Civil Law, Oil Gas Drilling or Automotive Engineering) will have higher salary ranges than sub-sectors with minimal entry requirements (such as Floristry or Nannies/Babysitters). Furthermore, we believe that some sectors such as ICT and Engineering will have a diverse range of job listing salaries. Since there are jobs within these sectors that could be low paying, or that could be in the top salary range among all job listings. In contrast, for sectors such as Call Centre Customer Service and Retail Consumer Products, it is assumed that the majority of job offerings within these sectors will be in the lower salary ranges due to the fact that the entry level of expertise for these positions is not very high. \n",
    "\n",
    "Our final hypothesis is in relation to the textual analysis to be conducted. Our aim is to extract skillsets from the job descriptions so that we can determine what are the most frequently requested skills for applicants within each sector. Since the requirement attribute of the provided dataset is not accurate in regard to skillsets (i.e. the textual data under the attribute contains other information than just job requirements), we believe that we will have to use the full description attribute to construct our own textual dataset regarding skillsets and requirements. Since this data is by nature dirty, and because we are not using any advanced natural language processing techniques due to the time constraints of the assessment, we hypothesise that the skillsets we find for each sector may not be perfectly accurate in regard to the context of skills required for a given sector. However, we still believe that we will be able to provide an informative study on the skillsets that are popular among different sectors and within the job market as a whole. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
